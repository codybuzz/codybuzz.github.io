# FastAI Course Lesson 7
## Collaborative Filtering
In Lesson 7 (collaborative filtering) of the FastAI course, I learnt the following:
1. Larger models:

| Pros | Cons |
|-|-|
| Have more parameters -> find more features -> more accurate models | Gradient calculations use up **a lot** of GPU memory.

* To use large models without running out of GPU memory, can use gradient accumulation (accumulating gradients over many batches).

2. Ensembling can be used with *different* models.

3. Multi-Target Models are models with multiple outputs.

4. Softmax function can be used to predict *probabilities* where a model has multiple categorical outputs:
$$\text{softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$$

5. Cross-Entropy can be used to determine loss for models with multiple targets.
$$-\sum_{j=1}^My_{j}\log(p(y_{j}))$$

6. You can split a loss function for a multi-target model into multiple loss functions for each target.

