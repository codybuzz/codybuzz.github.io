# FastAI Course Lesson 7
## Collaborative Filtering
In Lesson 7 (collaborative filtering) of the FastAI course, I learnt the following:
1. Larger models:

| Pros | Cons |
|-|-|
| Have more parameters -> find more features -> more accurate models | Gradient calculations use up **a lot** of GPU memory.

* To use large models without running out of GPU memory, can use gradient accumulation (accumulating gradients over many batches).

2. Ensembling can be used with *different* models.

3. Multi-Target Models are models with multiple outputs.
* Can be useful for predicting more than one output.
* Can also result in a better model for classifying an output than one specifically for that purpose (due to correlation between outputs).

4. Softmax function can be used to predict *probabilities* where a model has multiple categorical outputs:
$$\text{softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$$

5. Cross-Entropy can be used to determine loss for models with multiple targets.
$$-\sum_{j=1}^My_{j}\log(p(y_{j}))$$

6. You can split a loss function for a multi-target model into a separate loss function for each target.

7. Colaborative Filtering:
* Can use the dot product on latent factors for collaborative filtering.
* Can create latent factors (ways to group something).

8. L2 Regularization / Weight Decay
* Can be used to avoid overfitting.

