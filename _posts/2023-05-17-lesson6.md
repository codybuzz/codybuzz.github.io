# FastAI Course Lesson 6
## Random Forests
The key findings from Lesson 6 (Random Forests) of the FastAI course are the following:
1. Decision Trees:
* *gini* can be used to evaluate how good a split is. In other words - if you grab two items at random from the dataset, what are the chances they are from the same dataset.
* Decision trees have a *maximum depth*. Which refers to the number of splits.
* `sklearn` has a `DecisionTreeClassifier` that can be used to easily create these models.
* Can use a feature importance plot to see how important each feature is in making decision tree splits.
* An example of a decision tree can be seen below:
![](/images/decision_tree_example.PNG "An Example of a Decision Tree")

2. Random forests:
* Works by using bagging with many decision tree models.
* By taking the average of the predictions of the uncorrelated, unbiased models - better results *should* be obtained.
* `sklearn` has a `RandomForestClassifier` that can be used to easily create these models.
* More trees -> less error
* Can't overfit by adding too many trees (but trees with too much depth may still overfit).

3. Out of Bag (OOB) Error
* A way of measuring prediction error on the training set by only including in the calculation of a row's error trees where that row was not included in training.
* Allows us to see whether the model is overfitting, without needing a seperate validation set.

4. Partial Dependence:
* Considering only the effect of one input variable on the output and finding the relationship between them.

5. Gradient Boosting:
* Use many small models iteratively based on the error from the previous small model. Then take the sum of all models.
* Can overfit using boosting

6. Data Augmentation:
* Can pass multiple versions of inputs (i.e., different versions of an image with different distortions) and average the result.
* This is because sometimes a model will prefer one type of data to others.
