# Lesson 4
The key points that I have learnt from lesson 4 (natural language processing) of FastAI's course are the following:
1. To pass text into a model we need to do:
* **Tokenization** - Split each text up into words (*tokens*).
* **Numericalization** - Convert each word (*token*) into a number.

2. To adapt/use pretrained models we need to ensure that we tokenize the same way (for Hugging Face Transformers this can be done using `AutoTokenizer`)

3. Overfitting:
* Overfitting can occur when a model fits training data *too* closely, and is therefore influenced by noise in data.
* An overfitted model may be able to perfectly predict the training data, but might predict new data poorly.
* An example situation where a polynomial of degree 10 is fit to data from a quadratic (with some noise) can be seen below:
![](/images/overfit_example.PNG "Example of Overfitting to a Dataset")

4. Validation sets:
* A validation set can be used to check the performance of a model, as it is data that the model has not seen before.
* To avoid overfitting and get an accurate performance of a mode, the validation set must be chosen carefully (i.e., for time series, select a continuous *chunk* of time for the validation set, do not choose randomly from the entire dataset).
* To choose a good validation set, consider the conditions the model will see in real situations.

5. Metrics (i.e. accuracy) are __different__ from loss (i.e., mean absolute error).
* Metrics *can* be the same as the loss function however this is not usually the case.
* A loss function must be *smooth*, whereas metrics do not necessarily have to be.

6. Pearson correlation coefficient
* The coefficient is usually abbreviated by the single letter *r*.
* It is the most widely used measure of the degree of relationship between two variables.
* *r* is within the range `[-1 1]`, where -1 represents a perfect inverse correlation and 1 represents a perfect positive correlation.
* *r* is very sensitive to outliers.
