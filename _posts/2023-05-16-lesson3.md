# FastAI Course Lesson 3
The key things that I learnt in lesson 3 (neural net foundations) of the FastAI course include:
1. Loss function:
* In order to train a machine learning model, a loss function can be used. This loss function determines how *close* the model currently fits the dataset.
* By taking the gradient of the loss function, it can be determined how to change the parameters of the model to minimise the loss.

2. ReLU and approximating other functions:
* The Rectified Linear Unit (ReLU) is a piecewise-linear activation function that causes all negative inputs to become zero.
* Complex functions can be approximated by combining many ReLU functions - this also works in higher dimensions.
* An example of a ReLU function can be seen below:
![](/images/relu_example.png "Example of a ReLU Function")
*Note: example image obtained from [here](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)*

3. Learning rate:
* The learning rate is useful to control how quickly a neural network *learns*.
* By multiplying the gradient of the loss function with the learning rate, the distance to *step* in the direction of lower loss can be determined.
* A small learning rate will result in a model that trains slower, but a large learning rate could *overshoot/step* too far and end up increasing the loss.

4. You can use spreadsheets for building regression models and neural networks.

5. You can use matrix multiplication to complete training faster.
